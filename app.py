import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from openai import OpenAI
from datetime import datetime

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å¤‡è€ƒæ–°é—»AIåŠ©æ‰‹", layout="wide")

st.title("ğŸ“š è¡Œæµ‹ & æ ¡æ‹›ï¼šæ—¶æ”¿çƒ­ç‚¹ AI åŠ©æ‰‹")
st.markdown("è¯¥å·¥å…·è‡ªåŠ¨æŠ“å–äººæ°‘ç½‘æœ€æ–°åŠ¨æ€ï¼Œå¹¶è°ƒç”¨ AI æå–è€ƒè¯•é‡ç‚¹ã€‚")

# --- ä¾§è¾¹æ ï¼šé…ç½® ---
st.sidebar.header("âš™ï¸ é…ç½®ä¸­å¿ƒ")
api_key = 'sk-4c969651e5bf4a6491b9218b748f8647'
selected_channels = st.sidebar.multiselect(
    "é€‰æ‹©é‡‡é›†æ¿å—",
    ["ç»æµç§‘æŠ€", "ç¤¾ä¼šæ³•æ²»", "æ–‡æ—…ä½“è‚²", "å›½é™…æ–°é—»",'å…šæ”¿æ–°é—»','å…šæ”¿æ–°é—»','å†›äº‹æ–°é—»','å¤§æ¹¾åŒºæ–°é—»','å°æ¹¾æ–°é—»','æ•™è‚²æ–°é—»'],
    default=["ç»æµç§‘æŠ€", "ç¤¾ä¼šæ³•æ²»", "æ–‡æ—…ä½“è‚²", "å›½é™…æ–°é—»",'å…šæ”¿æ–°é—»','å…šæ”¿æ–°é—»','å†›äº‹æ–°é—»','å¤§æ¹¾åŒºæ–°é—»','å°æ¹¾æ–°é—»','æ•™è‚²æ–°é—»']
)

# é¢‘é“ URL æ˜ å°„
CHANNEL_MAP = {
    "ç»æµç§‘æŠ€": "http://finance.people.com.cn",
    "ç¤¾ä¼šæ³•æ²»": "http://society.people.com.cn",
    "æ–‡æ—…ä½“è‚²": "http://ent.people.com.cn",
    "å›½é™…æ–°é—»": "http://world.people.com.cn",
    'å…šæ”¿æ–°é—»': 'http://cpc.people.com.cn',
    'å†›äº‹æ–°é—»': 'http://military.people.com.cn',
    'å¤§æ¹¾åŒºæ–°é—»':'http://gba.people.cn',
    'å°æ¹¾æ–°é—»':'http://tw.people.com.cn',
    'æ•™è‚²æ–°é—»':'http://edu.people.com.cn',
}


# --- æ ¸å¿ƒé€»è¾‘ï¼šé‡‡é›† ---
def fetch_news(channels):
    all_news = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    for name in channels:
        url = CHANNEL_MAP[name]
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.encoding = resp.apparent_encoding
            soup = BeautifulSoup(resp.text, 'html.parser')
            # æŠ“å– 2026 å¹´é“¾æ¥
            links = soup.find_all('a', href=re.compile(r'/n1/2026/'))
            seen = set()
            for a in links[:50]:
                t = a.get_text(strip=True)
                if len(t) > 12 and t not in seen:
                    all_news.append({"æ¿å—": name, "æ–°é—»æ ‡é¢˜": t,
                                     "é“¾æ¥": url + a.get('href') if not a.get('href').startswith('http') else a.get(
                                         'href')})
                    seen.add(t)
        except Exception as e:
            st.error(f"æŠ“å– {name} å¤±è´¥: {e}")
    return pd.DataFrame(all_news)


# --- æ ¸å¿ƒé€»è¾‘ï¼šAI åˆ†æ ---
def get_ai_analysis(news_df, key):
    client = OpenAI(api_key= api_key, base_url="https://api.deepseek.com")
    titles = "\n".join([f"[{row['æ¿å—']}] {row['æ–°é—»æ ‡é¢˜']}" for _, row in news_df.iterrows()])

    prompt = (f"ä½ æ˜¯ä¸€åå…¬èŒè€ƒè¯•åŸ¹è®­ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹äººæ°‘ç½‘æ–°é—»æ ‡é¢˜ä¸­é€‰å‡º5-8ä¸ªæœ€é‡è¦çš„è€ƒç‚¹ã€‚è¦æ±‚ï¼š"
              f"é¦–å…ˆé€šè¿‡æ ‡é¢˜ä»¥åŠçˆ¬å–åˆ°çš„ç½‘å€æœç´¢å¹¶é˜…è¯»å®Œæ•´æŠ¥é“å†…å®¹ï¼Œåˆ—å‡ºè¿™æ¡æ–°é—»åœ¨äººæ°‘ç½‘å‘å‡ºçš„æ—¶é—´ï¼›å…¶æ¬¡æ ¹æ®æŠ¥é“å†…å®¹æ€»ç»“æç‚¼å‡ºå†…å®¹æ ¸å¿ƒï¼Œ"
              f"æŒ‰ç…§æ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ã€èµ·å› ã€ç»è¿‡ã€ç»“æœè¿™å…­è¦ç´ è¿›è¡Œæ€»ç»“ï¼Œæœ€åæŒ‡æ˜æ–°é—»æ˜¯å“ªä¸ªæ–¹é¢çš„ã€å¯èƒ½ä¸ä»€ä¹ˆè€ƒç‚¹æœ‰å…³ã€‚é’ˆå¯¹è¡Œæµ‹å¸¸è¯†ã€ç”³è®ºåŠé“¶è¡Œ/å›½ä¼æ ¡æ‹›ï¼›\n\næ–°é—»åˆ—è¡¨ï¼š\n{titles}")

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content


# --- é¡µé¢äº¤äº’é€»è¾‘ ---
if st.button("ğŸš€ å¼€å§‹é‡‡é›†å¹¶ç”Ÿæˆç®€æŠ¥"):
    if not selected_channels:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¿å—")
    else:
        with st.spinner("æ­£åœ¨çˆ¬å–å®æ—¶æ•°æ®..."):
            df = fetch_news(selected_channels)

        if not df.empty:
            col1, col2 = st.columns([1, 1])

            with col1:
                st.subheader("ğŸ“° å®æ—¶æ–°é—»åˆ—è¡¨")
                st.dataframe(df, use_container_width=True)
                # æä¾› CSV ä¸‹è½½
                csv = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½æ–°é—» CSV", data=csv, file_name="news.csv", mime="text/csv")

            with col2:
                st.subheader("ğŸ“ AI å¤‡è€ƒç²¾ç®€")
                if api_key:
                    with st.spinner("AI æ­£åœ¨æ·±åº¦æ€è€ƒè€ƒç‚¹..."):
                        analysis = get_ai_analysis(df, api_key)
                        st.markdown(analysis)
                else:
                    st.info("ğŸ’¡ è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ API Key ä»¥å¼€å¯ AI è€ƒç‚¹åˆ†æåŠŸèƒ½ã€‚")
        else:
            st.error("æœªèƒ½é‡‡é›†åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚")
